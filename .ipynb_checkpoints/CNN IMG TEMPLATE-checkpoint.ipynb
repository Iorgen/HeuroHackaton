{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch.utils.data as data\n",
    "import keras\n",
    "import random \n",
    "import itertools\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D\n",
    "from keras.preprocessing.image import img_to_array,array_to_img, load_img\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam, RMSprop,SGD\n",
    "from keras.preprocessing import image\n",
    "# from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "# from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from numpy import array\n",
    "from PIL import Image\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "base_skin_dir = os.path.join('', 'dataset')\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Graphics in SVG| format are more sharp and legible\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIMS = (100, 75, 3)\n",
    "# default loading\n",
    "def load_img(imagePath):\n",
    "    image = cv2.imread(imagePath, cv2.IMREAD_UNCHANGED)\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = img_to_array(image)\n",
    "    return image\n",
    "\n",
    "# return image with higher increase brightness\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    hsv = cv2.resize(hsv, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img_to_array(img)\n",
    "    return img\n",
    "\n",
    "# return thresholding featrue transformation\n",
    "def thresholding_feature(imagePath):\n",
    "    img_grey = cv2.imread(imagePath, cv2.IMREAD_GRAYSCALE)\n",
    "    img_grey = cv2.resize(img_grey, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    # # Adaptive Gaussian\n",
    "    img_grey = cv2.adaptiveThreshold(img_grey, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    # Otsu's thresholding after Gaussian filtering\n",
    "    blur = cv2.GaussianBlur(img_grey, (3, 3), 0)\n",
    "    ret3, img_binary = cv2.threshold(blur, 50, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # invert black = 255\n",
    "    # ret, thresh1 = cv2.threshold(img_binary, 157, 255, cv2.THRESH_BINARY_INV)\n",
    "    img_binary = img_to_array(img_binary)\n",
    "    return img_binary \n",
    "\n",
    "def load_atm_img(imagePath):\n",
    "    img = cv2.imread(imagePath,0)\n",
    "    img = cv2.resize(img, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    atm_img = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,2)\n",
    "    # TODO make it gray     \n",
    "    atm_img = img_to_array(atm_img)\n",
    "    return atm_img\n",
    "\n",
    "def load_img_with_cv2(imagePath, frth_channel = 'without'):\n",
    "    channel = False\n",
    "    image = load_img(imagePath)\n",
    "    sub_channel = 'no_channel'\n",
    "    if frth_channel == 'atm':\n",
    "        sub_channel = load_atm_img(imagePath)\n",
    "        channel = True\n",
    "    elif frth_channel == 'gray': \n",
    "        sub_channel = increase_brightness(image, 150)\n",
    "        channel = True\n",
    "    elif frth_channel == 'threshold':\n",
    "        sub_channel = thresholding_feature(imagePath)\n",
    "        channel = True\n",
    "    image = img_to_array(image)\n",
    "    if channel == True:\n",
    "        multiImage = np.concatenate((image, sub_channel), axis=2) \n",
    "        return multiImage\n",
    "    else: \n",
    "        return image\n",
    "    \n",
    "def load_gray(imagePath):\n",
    "    image = load_img(imagePath)\n",
    "    return increase_brightness(image, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for img_path in current_skin_data['path']:\n",
    "    data.append(load_img_with_cv2(img_path))\n",
    "labels = np.array(current_skin_data['dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "data = np.array(data)\n",
    "\n",
    "generate_labels = np.array(generate_labels)\n",
    "full_labels = np.concatenate((labels, generate_labels))\n",
    "\n",
    "generate_data = np.array(generate_data, dtype=np.uint8)\n",
    "full_data = np.concatenate((data, generate_data))\n",
    "\n",
    "full_data_mean = np.mean(full_data)\n",
    "full_data_std = np.std(full_data)\n",
    "full_data = (full_data - full_data_mean)/full_data_std\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "full_labels = mlb.fit_transform(full_labels)\n",
    "\n",
    "data_train, x_test, data_test, y_test = train_test_split(\n",
    "        full_data, full_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "        data_train, data_test, \n",
    "        test_size = 0.1, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics and optimizer for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_optimizer(optimizer):\n",
    "    if (optimizer == \"SGD\"):\n",
    "        return SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    elif (optimizer == \"RMSprop\"):\n",
    "        return RMSprop()\n",
    "    elif (optimizer == \"Adam\"):\n",
    "        return Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    else: \n",
    "        return Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "OPT = task_optimizer('RMSprop')\n",
    "OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(output_neurons = 7): \n",
    "    #  input shape using shape off test samples\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), input_shape=(100,75,3)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3)))\n",
    "    model.add(layers.Conv2D(128, (3, 3)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    model.add(layers.Dense(output_neurons))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer = OPT,\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "labels = np.array(labels)\n",
    "data = np.array(data)\n",
    "generate_labels = np.array(generate_labels)\n",
    "generate_data = np.array(generate_data, dtype=np.uint8)\n",
    "\n",
    "full_labels = np.concatenate((labels, generate_labels))\n",
    "full_data = np.concatenate((data, generate_data))\n",
    "\n",
    "full_data_mean = np.mean(full_data)\n",
    "full_data_std = np.std(full_data)\n",
    "full_data = (full_data - full_data_mean)/full_data_std\n",
    "print('Данные нормализованы')\n",
    "mlb = LabelBinarizer()\n",
    "full_labels = mlb.fit_transform(full_labels)\n",
    "\n",
    "data_train, x_test, data_test, y_test = train_test_split(\n",
    "    full_data, full_labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    data_train, data_test, \n",
    "    test_size = 0.1, random_state = 2)\n",
    "print('Данные разбиты на тестовые, валидационные и тренировочные')\n",
    "MD = get_model()\n",
    "history = MD.fit(x_train, y_train, epochs=20, batch_size=64 , validation_data=(x_validate, y_validate))\n",
    "loss_test, accuracy_test = MD.evaluate(x_test, y_test, verbose=1)\n",
    "loss_v, accuracy_v = MD.evaluate(x_validate, y_validate, verbose=1)\n",
    "loss_train, accuracy_train = MD.evaluate(x_train, y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = MD.evaluate(x_test, y_test, verbose=1)\n",
    "loss_v, accuracy_v = MD.evaluate(x_validate, y_validate, verbose=1)\n",
    "print(\"Validation: accuracy = %f  ;  loss = %f  \"  % (accuracy_v, loss_v))\n",
    "print(\"Test: accuracy = %f  ;  loss = %f ;\" % (accuracy, loss))\n",
    "# model.save_weights(\"weights.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show graphic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = MD.predict(x_validate)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_validate,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    " \n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(7)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
